{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_9mSMZqLzXb"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
    "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
    "import kagglehub\n",
    "kagglehub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1stAQ2DNLzXe"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "ariel_data_challenge_2024_path = kagglehub.competition_download('ariel-data-challenge-2024')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFZN-pOtLzXf"
   },
   "source": [
    "# Calibrating and Time Binning Astronomical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkqfMM3OLzXh"
   },
   "source": [
    "**UPDATE 14.08**: We have updated the calibration steps in the notebook. We understand that some of you have been using the old calibration procedure. The old procedures still provide a good estimate of the transit depth over different wavelengths, but as competition hosts we understand the difficulty to accurately calibrate the data product (even for us, it is still a learning journey, afterall, we are preparing for the mission), and we want to make sure you have the most updated knowledge on the calibration pipeline, so that it helps your journey in tackling this challenge. We will explain more in our discussion\n",
    "\n",
    "**UPDATE 29.08**: We have added 0.1s to the integration time for both AIRS and FGS observations. They are important when accounting for the contributing of dark frames when calibration the image. The modification, however, should not affect too much of the calibrated product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkzigUZMLzXh"
   },
   "source": [
    "Data reduction is crucial in astronomical observations, and this challenge is no exception. This notebook outlines essential calibration steps typically employed by astronomers to mitigate noise in data.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- The notebook guides participants through pre-processing data and saving it in a more convenient, lighter format.\n",
    "- If you plan to use the baseline models (which will be released soon), you must run this notebook first before training.\n",
    "\n",
    "Important note: While these steps help reduce noise and data size, they may not be the most effective approach for achieving the optimal model for this challenge. Participants are encouraged to explore alternative methods that could yield better results.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbsub-a_LzXh"
   },
   "source": [
    "**Acknowledgement**: This notebook is prepared by Angèle Syty and Virginie Batista (IAP), with support from Andrea Bocchieri, Orphée Faucoz (CNES), Lorenzo V. Mugnai (Cardiff University & UCL), Tara Tahseen (UCL), Gordon Yip (UCL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-DApfw7LzXi"
   },
   "source": [
    "Last modified: 29 Aug 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T12:32:19.428206Z",
     "iopub.status.busy": "2024-08-27T12:32:19.427734Z",
     "iopub.status.idle": "2024-08-27T12:32:21.448089Z",
     "shell.execute_reply": "2024-08-27T12:32:21.446853Z",
     "shell.execute_reply.started": "2024-08-27T12:32:19.428172Z"
    },
    "id": "5N5KBYBwLzXi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import glob\n",
    "from astropy.stats import sigma_clip\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BWgGFE2LzXj"
   },
   "source": [
    "Below, we define the corrections we want to apply, the size of the data chunks and the different path used to import data and save the light ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:30.960116Z",
     "iopub.status.busy": "2024-08-14T15:43:30.959386Z",
     "iopub.status.idle": "2024-08-14T15:43:30.966341Z",
     "shell.execute_reply": "2024-08-14T15:43:30.965158Z",
     "shell.execute_reply.started": "2024-08-14T15:43:30.960071Z"
    },
    "id": "9pBuuglJLzXj"
   },
   "outputs": [],
   "source": [
    "\n",
    "path_folder = '/kaggle/input/ariel-data-challenge-2024/' # path to the folder containing the data\n",
    "path_out = '/kaggle/tmp/data_light_raw/' # path to the folder to store the light data\n",
    "output_dir = '/kaggle/tmp/data_light_raw/' # path for the output directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wQD2oYGLzXk"
   },
   "source": [
    "If the *path_out* folder doesn't exist yet, it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:30.968151Z",
     "iopub.status.busy": "2024-08-14T15:43:30.967731Z",
     "iopub.status.idle": "2024-08-14T15:43:30.980039Z",
     "shell.execute_reply": "2024-08-14T15:43:30.978927Z",
     "shell.execute_reply.started": "2024-08-14T15:43:30.968106Z"
    },
    "id": "JkqR7-M7LzXk"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(path_out):\n",
    "    os.makedirs(path_out)\n",
    "    print(f\"Directory {path_out} created.\")\n",
    "else:\n",
    "    print(f\"Directory {path_out} already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUvbA4fTLzXk"
   },
   "source": [
    "**Data import:**\n",
    "\n",
    " The files are imported by chunks of size 'CHUNK_SIZE' to avoid exceeding the memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:30.982032Z",
     "iopub.status.busy": "2024-08-14T15:43:30.981646Z",
     "iopub.status.idle": "2024-08-14T15:43:30.991466Z",
     "shell.execute_reply": "2024-08-14T15:43:30.990131Z",
     "shell.execute_reply.started": "2024-08-14T15:43:30.982002Z"
    },
    "id": "xvgS0PB7LzXk"
   },
   "outputs": [],
   "source": [
    "CHUNKS_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:38:41.270597Z",
     "iopub.status.busy": "2024-08-02T14:38:41.270117Z",
     "iopub.status.idle": "2024-08-02T14:38:41.303633Z",
     "shell.execute_reply": "2024-08-02T14:38:41.30235Z",
     "shell.execute_reply.started": "2024-08-02T14:38:41.270558Z"
    },
    "id": "MpiDz16qLzXl"
   },
   "source": [
    "## Step 1: Analog-to-Digital Conversion\n",
    "\n",
    "The Analog-to-Digital Conversion (adc) is performed by the detector to convert the pixel voltage into an integer number. We revert this operation by using the gain and offset for the calibration files 'train_adc_info.csv'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:30.995261Z",
     "iopub.status.busy": "2024-08-14T15:43:30.994865Z",
     "iopub.status.idle": "2024-08-14T15:43:31.005848Z",
     "shell.execute_reply": "2024-08-14T15:43:31.004311Z",
     "shell.execute_reply.started": "2024-08-14T15:43:30.99523Z"
    },
    "id": "fIBh9bVzLzXl"
   },
   "outputs": [],
   "source": [
    "def ADC_convert(signal, gain, offset):\n",
    "    signal = signal.astype(np.float64)\n",
    "    signal /= gain\n",
    "    signal += offset\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1bFXZg-LzXl"
   },
   "source": [
    "## Step 2: Mask hot/dead pixel\n",
    "The dead pixels map is a map of the pixels that do not respond to light and, thus, can’t be accounted for any calculation. In all these frames the dead pixels are masked using python masked arrays. The bad pixels are thus masked but left uncorrected. Some methods can be used to correct bad-pixels but this task, if needed, is left to the participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.007782Z",
     "iopub.status.busy": "2024-08-14T15:43:31.007316Z",
     "iopub.status.idle": "2024-08-14T15:43:31.019851Z",
     "shell.execute_reply": "2024-08-14T15:43:31.018603Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.007745Z"
    },
    "id": "KvMy1fhLLzXl"
   },
   "outputs": [],
   "source": [
    "def mask_hot_dead(signal, dead, dark):\n",
    "    hot = sigma_clip(\n",
    "        dark, sigma=5, maxiters=5\n",
    "    ).mask\n",
    "    hot = np.tile(hot, (signal.shape[0], 1, 1))\n",
    "    dead = np.tile(dead, (signal.shape[0], 1, 1))\n",
    "    signal = np.ma.masked_where(dead, signal)\n",
    "    signal = np.ma.masked_where(hot, signal)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6rACD8MLzXm"
   },
   "source": [
    "## Step 2: linearity Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2JYYBYLzXm"
   },
   "source": [
    "\n",
    "\n",
    "**Non-linearity of pixels' response:**\n",
    "\n",
    "The non-linearity of the pixels’ response can be explained as capacitive leakage on the readout electronics of each pixel during the integration time. The number of electrons in the well is proportional to the number of photons that hit the pixel, with a quantum efficiency coefficient. However, the response of the pixel is not linear with the number of electrons in the well. This effect can be described by a polynomial function of the number of electrons actually in the well. The data is provided with calibration files linear_corr.parquet that are the coefficients of the inverse polynomial function and can be used to correct this non-linearity effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.02172Z",
     "iopub.status.busy": "2024-08-14T15:43:31.021396Z",
     "iopub.status.idle": "2024-08-14T15:43:31.033814Z",
     "shell.execute_reply": "2024-08-14T15:43:31.032592Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.021693Z"
    },
    "id": "RTxROW_HLzXm"
   },
   "outputs": [],
   "source": [
    "def apply_linear_corr(linear_corr,clean_signal):\n",
    "    linear_corr = np.flip(linear_corr, axis=0)\n",
    "    for x, y in itertools.product(\n",
    "                range(clean_signal.shape[1]), range(clean_signal.shape[2])\n",
    "            ):\n",
    "        poli = np.poly1d(linear_corr[:, x, y])\n",
    "        clean_signal[:, x, y] = poli(clean_signal[:, x, y])\n",
    "    return clean_signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHUZOmmELzXm"
   },
   "source": [
    "## Step 3: dark current subtraction\n",
    "\n",
    "The data provided include calibration for dark current estimation, which can be used to pre-process the observations. Dark current represents a constant signal that accumulates in each pixel during the integration time, independent of the incoming light. To obtain the corrected image, the following conventional approach is applied: The data provided include calibration files such as dark frames or dead pixels' maps. They can be used to pre-process the observations. The dark frame is a map of the detector response to a very short exposure time, to correct for the dark current of the detector.\n",
    "$$\\text{image - dark} \\times \\Delta t $$\n",
    "The corrected image is conventionally obtained via the following: where the dark current map is first corrected for the dead pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.035887Z",
     "iopub.status.busy": "2024-08-14T15:43:31.035536Z",
     "iopub.status.idle": "2024-08-14T15:43:31.045441Z",
     "shell.execute_reply": "2024-08-14T15:43:31.044309Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.035858Z"
    },
    "id": "kCc1hmt8LzXm"
   },
   "outputs": [],
   "source": [
    "def clean_dark(signal, dead, dark, dt):\n",
    "\n",
    "    dark = np.ma.masked_where(dead, dark)\n",
    "    dark = np.tile(dark, (signal.shape[0], 1, 1))\n",
    "\n",
    "    signal -= dark* dt[:, np.newaxis, np.newaxis]\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoC4MFFSLzXn"
   },
   "source": [
    "## Step 4: Get Correlated Double Sampling (CDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWsLPAOVLzXn"
   },
   "source": [
    "The science frames are alternating between the start of the exposure and the end of the exposure. The lecture scheme is a ramp with a double sampling, called Correlated Double Sampling (CDS), the detector is read twice, once at the start of the exposure and once at the end of the exposure. The final CDS is the difference (End of exposure) - (Start of exposure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.047248Z",
     "iopub.status.busy": "2024-08-14T15:43:31.046913Z",
     "iopub.status.idle": "2024-08-14T15:43:31.057686Z",
     "shell.execute_reply": "2024-08-14T15:43:31.05647Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.04722Z"
    },
    "id": "11JlE77oLzXn"
   },
   "outputs": [],
   "source": [
    "def get_cds(signal):\n",
    "    cds = signal[:,1::2,:,:] - signal[:,::2,:,:]\n",
    "    return cds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3v96YAaLzXn"
   },
   "source": [
    "## Step 5 (Optional): Time Binning\n",
    "This step is performed mianly to save space. Time series observations are binned together at specified frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.059681Z",
     "iopub.status.busy": "2024-08-14T15:43:31.05926Z",
     "iopub.status.idle": "2024-08-14T15:43:31.070098Z",
     "shell.execute_reply": "2024-08-14T15:43:31.068951Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.059648Z"
    },
    "id": "DxZ8jUk-LzXn"
   },
   "outputs": [],
   "source": [
    "def bin_obs(cds_signal,binning):\n",
    "    cds_transposed = cds_signal.transpose(0,1,3,2)\n",
    "    cds_binned = np.zeros((cds_transposed.shape[0], cds_transposed.shape[1]//binning, cds_transposed.shape[2], cds_transposed.shape[3]))\n",
    "    for i in range(cds_transposed.shape[1]//binning):\n",
    "        cds_binned[:,i,:,:] = np.sum(cds_transposed[:,i*binning:(i+1)*binning,:,:], axis=1)\n",
    "    return cds_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrkrFxCRLzXn"
   },
   "source": [
    "## Step 6: Flat Field Correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXEQO324LzXo"
   },
   "source": [
    "The flat field is a map of the detector response to uniform illumination, to correct for the pixel-to-pixel variations of the detector, for example the different quantum efficiencies of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.07204Z",
     "iopub.status.busy": "2024-08-14T15:43:31.071559Z",
     "iopub.status.idle": "2024-08-14T15:43:31.087923Z",
     "shell.execute_reply": "2024-08-14T15:43:31.086571Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.072004Z"
    },
    "id": "nSQeLhPJLzXo"
   },
   "outputs": [],
   "source": [
    "def correct_flat_field(flat,dead, signal):\n",
    "    flat = flat.transpose(1, 0)\n",
    "    dead = dead.transpose(1, 0)\n",
    "    flat = np.ma.masked_where(dead, flat)\n",
    "    flat = np.tile(flat, (signal.shape[0], 1, 1))\n",
    "    signal = signal / flat\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjpJXYXjLzXo"
   },
   "source": [
    "# Calibrating all training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5yHGzC0LzXo"
   },
   "source": [
    "you can choose to correct the non-linearity of the pixels' response, to apply flat field, dark and dead map or to leave the data unchanged. The observations are binned in time by group of 30 frames for AIRS and 360 frames for FGS1, to obtain a lighter data-cube, easier to use. The images are cut along the wavelength axis between pixels 39 and 321, so that the 282 pixels left in the wavelength dimension match the last 282 targets' points, from AIRS. The 283rd targets' point is the one for FGS1 that will be added later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.090081Z",
     "iopub.status.busy": "2024-08-14T15:43:31.089594Z",
     "iopub.status.idle": "2024-08-14T15:43:31.105731Z",
     "shell.execute_reply": "2024-08-14T15:43:31.104091Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.090039Z"
    },
    "id": "REcMiQorLzXo"
   },
   "outputs": [],
   "source": [
    "## we will start by getting the index of the training data:\n",
    "def get_index(files,CHUNKS_SIZE ):\n",
    "    index = []\n",
    "    for file in files :\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name.split('_')[0] == 'AIRS-CH0' and file_name.split('_')[1] == 'signal.parquet':\n",
    "            file_index = os.path.basename(os.path.dirname(file))\n",
    "            index.append(int(file_index))\n",
    "    index = np.array(index)\n",
    "    index = np.sort(index)\n",
    "    # credit to DennisSakva\n",
    "    index=np.array_split(index, len(index)//CHUNKS_SIZE)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:43:31.108094Z",
     "iopub.status.busy": "2024-08-14T15:43:31.107676Z",
     "iopub.status.idle": "2024-08-14T15:45:33.985108Z",
     "shell.execute_reply": "2024-08-14T15:45:33.983191Z",
     "shell.execute_reply.started": "2024-08-14T15:43:31.108063Z"
    },
    "id": "yHUCORpRLzXp"
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(path_folder + 'train/', '*/*'))\n",
    "\n",
    "index = get_index(files[:22],CHUNKS_SIZE)  ## 48 is hardcoded here but please feel free to remove it if you want to do it for the entire dataset\n",
    "\n",
    "train_adc_info = pd.read_csv(os.path.join(path_folder, 'train_adc_info.csv'))\n",
    "train_adc_info = train_adc_info.set_index('planet_id')\n",
    "axis_info = pd.read_parquet(os.path.join(path_folder,'axis_info.parquet'))\n",
    "DO_MASK = True\n",
    "DO_THE_NL_CORR = False\n",
    "DO_DARK = True\n",
    "DO_FLAT = True\n",
    "TIME_BINNING = True\n",
    "\n",
    "cut_inf, cut_sup = 39, 321\n",
    "l = cut_sup - cut_inf\n",
    "\n",
    "for n, index_chunk in enumerate(tqdm(index)):\n",
    "    AIRS_CH0_clean = np.zeros((CHUNKS_SIZE, 11250, 32, l))\n",
    "    FGS1_clean = np.zeros((CHUNKS_SIZE, 135000, 32, 32))\n",
    "\n",
    "    for i in range (CHUNKS_SIZE) :\n",
    "        df = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_signal.parquet'))\n",
    "        signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 356))\n",
    "        gain = train_adc_info['AIRS-CH0_adc_gain'].loc[index_chunk[i]]\n",
    "        offset = train_adc_info['AIRS-CH0_adc_offset'].loc[index_chunk[i]]\n",
    "        signal = ADC_convert(signal, gain, offset)\n",
    "        dt_airs = axis_info['AIRS-CH0-integration_time'].dropna().values\n",
    "        dt_airs[1::2] += 0.1\n",
    "        chopped_signal = signal[:, :, cut_inf:cut_sup]\n",
    "        del signal, df\n",
    "\n",
    "        # CLEANING THE DATA: AIRS\n",
    "        flat = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        dark = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/dark.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        dead_airs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/dead.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        linear_corr = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 356))[:, :, cut_inf:cut_sup]\n",
    "\n",
    "        if DO_MASK:\n",
    "            chopped_signal = mask_hot_dead(chopped_signal, dead_airs, dark)\n",
    "            AIRS_CH0_clean[i] = chopped_signal\n",
    "        else:\n",
    "            AIRS_CH0_clean[i] = chopped_signal\n",
    "\n",
    "        if DO_THE_NL_CORR:\n",
    "            linear_corr_signal = apply_linear_corr(linear_corr,AIRS_CH0_clean[i])\n",
    "            AIRS_CH0_clean[i,:, :, :] = linear_corr_signal\n",
    "        del linear_corr\n",
    "\n",
    "        if DO_DARK:\n",
    "            cleaned_signal = clean_dark(AIRS_CH0_clean[i], dead_airs, dark, dt_airs)\n",
    "            AIRS_CH0_clean[i] = cleaned_signal\n",
    "        else:\n",
    "            pass\n",
    "        del dark\n",
    "\n",
    "        df = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_signal.parquet'))\n",
    "        fgs_signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 32))\n",
    "\n",
    "        FGS1_gain = train_adc_info['FGS1_adc_gain'].loc[index_chunk[i]]\n",
    "        FGS1_offset = train_adc_info['FGS1_adc_offset'].loc[index_chunk[i]]\n",
    "\n",
    "        fgs_signal = ADC_convert(fgs_signal, FGS1_gain, FGS1_offset)\n",
    "        dt_fgs1 = np.ones(len(fgs_signal))*0.1\n",
    "        dt_fgs1[1::2] += 0.1\n",
    "        chopped_FGS1 = fgs_signal\n",
    "        del fgs_signal, df\n",
    "\n",
    "        # CLEANING THE DATA: FGS1\n",
    "        flat = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        dark = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/dark.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        dead_fgs1 = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/dead.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        linear_corr = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 32))\n",
    "\n",
    "        if DO_MASK:\n",
    "            chopped_FGS1 = mask_hot_dead(chopped_FGS1, dead_fgs1, dark)\n",
    "            FGS1_clean[i] = chopped_FGS1\n",
    "        else:\n",
    "            FGS1_clean[i] = chopped_FGS1\n",
    "\n",
    "        if DO_THE_NL_CORR:\n",
    "            linear_corr_signal = apply_linear_corr(linear_corr,FGS1_clean[i])\n",
    "            FGS1_clean[i,:, :, :] = linear_corr_signal\n",
    "        del linear_corr\n",
    "\n",
    "        if DO_DARK:\n",
    "            cleaned_signal = clean_dark(FGS1_clean[i], dead_fgs1, dark,dt_fgs1)\n",
    "            FGS1_clean[i] = cleaned_signal\n",
    "        else:\n",
    "            pass\n",
    "        del dark\n",
    "\n",
    "    # SAVE DATA AND FREE SPACE\n",
    "    AIRS_cds = get_cds(AIRS_CH0_clean)\n",
    "    FGS1_cds = get_cds(FGS1_clean)\n",
    "\n",
    "    del AIRS_CH0_clean, FGS1_clean\n",
    "\n",
    "    ## (Optional) Time Binning to reduce space\n",
    "    if TIME_BINNING:\n",
    "        AIRS_cds_binned = bin_obs(AIRS_cds,binning=30)\n",
    "        FGS1_cds_binned = bin_obs(FGS1_cds,binning=30*12)\n",
    "    else:\n",
    "        AIRS_cds = AIRS_cds.transpose(0,1,3,2) ## this is important to make it consistent for flat fielding, but you can always change it\n",
    "        AIRS_cds_binned = AIRS_cds\n",
    "        FGS1_cds = FGS1_cds.transpose(0,1,3,2)\n",
    "        FGS1_cds_binned = FGS1_cds\n",
    "\n",
    "    del AIRS_cds, FGS1_cds\n",
    "\n",
    "    for i in range (CHUNKS_SIZE):\n",
    "        flat_airs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "        flat_fgs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "        if DO_FLAT:\n",
    "            corrected_AIRS_cds_binned = correct_flat_field(flat_airs,dead_airs, AIRS_cds_binned[i])\n",
    "            AIRS_cds_binned[i] = corrected_AIRS_cds_binned\n",
    "            corrected_FGS1_cds_binned = correct_flat_field(flat_fgs,dead_fgs1, FGS1_cds_binned[i])\n",
    "            FGS1_cds_binned[i] = corrected_FGS1_cds_binned\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    ## save data\n",
    "    np.save(os.path.join(path_out, 'AIRS_clean_train_{}.npy'.format(n)), AIRS_cds_binned)\n",
    "    np.save(os.path.join(path_out, 'FGS1_train_{}.npy'.format(n)), FGS1_cds_binned)\n",
    "    del AIRS_cds_binned\n",
    "    del FGS1_cds_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiQoumXjLzXp"
   },
   "source": [
    "Once all the chunks are saved, we concatenate them back in a single dataset. This step is simply to save HDD space, modify it as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:45:33.992675Z",
     "iopub.status.busy": "2024-08-14T15:45:33.992081Z",
     "iopub.status.idle": "2024-08-14T15:45:34.116287Z",
     "shell.execute_reply": "2024-08-14T15:45:34.114991Z",
     "shell.execute_reply.started": "2024-08-14T15:45:33.992614Z"
    },
    "id": "2z1cDiP-LzXp"
   },
   "outputs": [],
   "source": [
    "def load_data (file, chunk_size, nb_files) :\n",
    "    data0 = np.load(file + '_0.npy')\n",
    "    data_all = np.zeros((nb_files*chunk_size, data0.shape[1], data0.shape[2], data0.shape[3]))\n",
    "    data_all[:chunk_size] = data0\n",
    "    for i in range (1, nb_files) :\n",
    "        data_all[i*chunk_size:(i+1)*chunk_size] = np.load(file + '_{}.npy'.format(i))\n",
    "    return data_all\n",
    "\n",
    "data_train = load_data(path_out + 'AIRS_clean_train', CHUNKS_SIZE, len(index))\n",
    "data_train_FGS = load_data(path_out + 'FGS1_train', CHUNKS_SIZE, len(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:45:34.118141Z",
     "iopub.status.busy": "2024-08-14T15:45:34.117758Z",
     "iopub.status.idle": "2024-08-14T15:45:34.207721Z",
     "shell.execute_reply": "2024-08-14T15:45:34.20643Z",
     "shell.execute_reply.started": "2024-08-14T15:45:34.11811Z"
    },
    "id": "ACIHmu6ALzXp"
   },
   "outputs": [],
   "source": [
    "np.save('/kaggle/working/' + 'data_train.npy', data_train)\n",
    "np.save('/kaggle/working/' + 'data_train_FGS.npy', data_train_FGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsZ0AnPtLzXq"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agw7G1vhLzXv"
   },
   "source": [
    "Let us checks that everything went well during the data import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:45:34.209816Z",
     "iopub.status.busy": "2024-08-14T15:45:34.209352Z",
     "iopub.status.idle": "2024-08-14T15:45:34.217862Z",
     "shell.execute_reply": "2024-08-14T15:45:34.216717Z",
     "shell.execute_reply.started": "2024-08-14T15:45:34.209763Z"
    },
    "id": "3ubesqQSLzXw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Shape of the training datasset: \\t')\n",
    "print('\\n For AIRS-CH0:', data_train.shape)\n",
    "print('\\n For FGS1:', data_train_FGS.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjOd79ptLzXw"
   },
   "source": [
    "Plot of some images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:45:34.219788Z",
     "iopub.status.busy": "2024-08-14T15:45:34.21942Z",
     "iopub.status.idle": "2024-08-14T15:45:34.575938Z",
     "shell.execute_reply": "2024-08-14T15:45:34.574691Z",
     "shell.execute_reply.started": "2024-08-14T15:45:34.219756Z"
    },
    "id": "pJ40YXHJLzXw"
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_train_FGS[-1,50,:,:].T, aspect = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3HT4wLLLzXw"
   },
   "source": [
    "Plot of some light-curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T15:45:34.577632Z",
     "iopub.status.busy": "2024-08-14T15:45:34.5773Z",
     "iopub.status.idle": "2024-08-14T15:45:34.921121Z",
     "shell.execute_reply": "2024-08-14T15:45:34.919856Z",
     "shell.execute_reply.started": "2024-08-14T15:45:34.577603Z"
    },
    "id": "4K1QOGcaLzXx"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(data_train)) :\n",
    "    light_curve = data_train[i,:,:,:].sum(axis=(1,2))\n",
    "    plt.plot(light_curve/light_curve.mean(), '-', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Time (frame index)')\n",
    "plt.ylabel('Normalized flux in the frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPi5Gc76LzXx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "[UPDATE]Calibrating and Binning Astronomical Data",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9188054,
     "sourceId": 70367,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
